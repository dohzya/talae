# Create a file '.env.local' for local development overrides

PORT_BACKEND=8000
HOST_BACKEND=http://localhost:$PORT_BACKEND
PORT_FRONTEND=5173
HOST_FRONTEND=http://localhost:$PORT_FRONTEND

# LLM Configuration
# Default: Ollama with ministral-3:8b (good for local development)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=ministral-3:8b
LLM_PROVIDER=ollama

# To use OpenAI instead:
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-...
# OPENAI_MODEL=gpt-4o-mini

# Database
DATABASE_PATH=./data/talae.db

# Application
NODE_ENV=development

# Note: List available Ollama models with: ollama list
# Pull ministral-3:8b with: ollama pull ministral-3:8b
